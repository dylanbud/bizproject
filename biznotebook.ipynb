{"cells":[{"cell_type":"markdown","id":"c05d1067","metadata":{},"source":["## 4chan /biz/ Scraper ##\n","\n","This notebook will scrape the /biz/ board on 4chan at the moment, and download all images associated with the 200 active threads on the board.\n","\n","The intent behind this scraping is to feed the reply/subject text data into a machine learning model and attempt to recrate posts. I would also like to do basic analysis on the text data.\n","\n","The image data is collected and stored in /imgs/, I'd like for the images to be the input for both a Fazle Rabbitrained classification neural network and a trained generative neural network."]},{"cell_type":"code","execution_count":null,"id":"5b172e0c","metadata":{},"outputs":[],"source":["import time\n","import pandas as pd\n","from urllib.request import urlopen\n","import json\n","from bs4 import BeautifulSoup\n","from urllib.error import HTTPError\n","import requests\n","from PIL import Image\n","import re\n","import random\n","\n","\n","# Collect general information about boards\n","\n","url = (('https://a.4cdn.org/boards.json'))\n","\n","data = get_jsonparsed_data(url)\n","\n","\n","board_names = []\n","board_titles = []\n","combined = []\n","pages = []\n","\n","for i in range(0, 78):\n","    name = data['boards'][i]['board']\n","    title = data['boards'][i]['title']\n","    page = data['boards'][i]['pages']\n","    board_names.append(name)\n","    board_titles.append(title)\n","    pages.append(page)\n","    combination = name + \" \" + title\n","    combined.append(combination)\n","\n","board_df = pd.DataFrame({'Name' : board_names, 'Title' : board_titles, 'Combined' : combined, 'Pages' : pages})\n","\n","board_df"]},{"cell_type":"code","execution_count":null,"id":"99bd18ba","metadata":{},"outputs":[],"source":["# prompt user for input\n","board = input(\"Enter a board: \")\n","print(board)"]},{"cell_type":"code","execution_count":null,"id":"824a476c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":2031,"status":"ok","timestamp":1639433463624,"user":{"displayName":"Dylan Budnick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhX3fJsQsvqzuZK06O7pK0XWM30sB0MVv0a0BZ3Hg=s64","userId":"08607160935893473835"},"user_tz":480},"id":"824a476c","outputId":"790ffc4b-1a51-4d27-d5ae-0c00d4807881"},"outputs":[],"source":["# Pulling Board information\n","\n","\n","def get_jsonparsed_data(url):\n","    response = urlopen(url)\n","    data = response.read().decode(\"utf-8\")\n","    return json.loads(data)\n","\n","\n","def get_numbers(df, i):  \n","    postno = df['Post Number'][i]\n","    replies = df['Replies'][i]\n","    return(postno, replies)\n","\n","dataframe = pd.DataFrame(columns=['Subject', 'Comment', 'Post Number', 'Replies', 'reply_list', 'filename', 'tim_list', 'ext_list', 'combined'])\n","for i in range(10):\n","    i=i+1\n","    url = ((\"https://a.4cdn.org/\") + str(board) + \"/\" + str(i) + '.json')\n","    data = get_jsonparsed_data(url)\n","\n","    list = len(data['threads'])\n","   \n","    for i in range(0, list): \n","      try:\n","          comment = data['threads'][i]['posts'][0]['com']\n","          subject = data['threads'][i]['posts'][0]['sub']\n","          postno = data['threads'][i]['posts'][0]['no']\n","          replies = data['threads'][i]['posts'][0]['replies']\n","      except KeyError:\n","          subject = 'No subject'\n","          postno = data['threads'][i]['posts'][0]['no']\n","          replies = data['threads'][i]['posts'][0]['replies']\n","      except KeyError:\n","          comment = data['threads'][i]['posts'][0]['sub']\n","      dataframe = dataframe.append({'Subject':subject, 'Comment':comment, 'Post Number':postno, 'Replies':replies}, ignore_index=True)\n","    time.sleep(1)\n","    i=i+1\n","\n","dataframe = dataframe.fillna(0)"]},{"cell_type":"markdown","id":"1c117af2","metadata":{},"source":["Above code produces dataframe seen below"]},{"cell_type":"code","execution_count":null,"id":"dd1c8f24","metadata":{},"outputs":[],"source":["dataframe.head(5)"]},{"cell_type":"markdown","id":"7df1214d","metadata":{},"source":["### Grab reply text, image md5 hash (name), and img extension #\n","\n","This function goes in and grabs all text and image data from replies in a given thread. It then drops the replies into a list in \"reply_list\", and drops the image name (it's md5 hash) and extension in the repsective columns\n","called \"tim_list\" and \"ext_list\". The \"combined\" column is the result of pairing the img name (md5) with the correlating extension.\n"]},{"cell_type":"code","execution_count":null,"id":"KOubLYwT12YX","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1639434665151,"user":{"displayName":"Dylan Budnick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhX3fJsQsvqzuZK06O7pK0XWM30sB0MVv0a0BZ3Hg=s64","userId":"08607160935893473835"},"user_tz":480},"id":"KOubLYwT12YX","outputId":"02a4dadf-68f6-4936-8767-3d0afe658380"},"outputs":[],"source":["for i in range(0,len(dataframe)):\n","\n","  try:\n","    postno, replies = get_numbers(dataframe, i) \n","    url = ((\"https://a.4cdn.org/\" + str(board) + \"/thread/\") + str(postno) + '.json')\n","    data = get_jsonparsed_data(url)\n","    replies_text = []\n","    extensions = []\n","    images = []\n","    combined = []\n","    filenames = []\n","    for j in range(0,replies):\n","      try:\n","        reply = data['posts'][j]['com']\n","        img = data['posts'][j]['tim']\n","        ext = data['posts'][j]['ext']\n","        #file = data['posts'][i]['filename']\n","        #filenames.append(file)\n","        replies_text.append(reply)\n","        images.append(str(img))\n","        extensions.append(str(ext))\n","        combined.append(str(img)+str(ext))\n","        dataframe['reply_list'][i] = replies_text\n","        dataframe['tim_list'][i] = images\n","        dataframe['ext_list'][i] = extensions\n","        dataframe['combined'][i] = combined\n","        #dataframe['filename'][i] = filenames\n","      except KeyError:\n","        pass\n","      except IndexError: \n","        pass\n","  except HTTPError:\n","    dataframe['reply_list'][i] = '404'\n","    dataframe['tim_list'][i] = '404'\n","    dataframe['ext_list'][i] = '404'\n","  else:\n","    pass\n","time.sleep(1)\n"]},{"cell_type":"markdown","id":"2caad79f","metadata":{},"source":["The above will format the dataframe into the format below:"]},{"cell_type":"code","execution_count":null,"id":"IMo6sHmslHZO","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":461,"status":"ok","timestamp":1638364518028,"user":{"displayName":"Dylan Budnick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhX3fJsQsvqzuZK06O7pK0XWM30sB0MVv0a0BZ3Hg=s64","userId":"08607160935893473835"},"user_tz":480},"id":"IMo6sHmslHZO","outputId":"7832fa3f-8809-4511-a054-1ab9c63dc08a"},"outputs":[],"source":["dataframe"]},{"cell_type":"code","execution_count":null,"id":"ec9bf4c1","metadata":{},"outputs":[],"source":["# # DOWNLOAD ALL IMAGES FOR THREAD/IN THREADS\n","\n","# links = []\n","# newlinks = []\n","# dataframe['combined'] = dataframe['combined'].fillna(0)\n","# combined = dataframe['combined']\n","\n","# for i in range(0, len(combined)):\n","#     if combined[i] == 0:\n","#         links.append('No image')\n","#     elif combined[i] != 0:\n","#         links = combined[i]\n","#         for i in links:\n","#             newlinks.append(i)\n","#             imgURL = (\"https://i.4cdn.org/biz/\") + str(i)\n","#             name = str(i)\n","#             print(imgURL)\n","#             r = requests.get(imgURL)\n","#             with open('imgs/'+name, 'wb') as f:\n","#                 f.write(r.content)\n","#             time.sleep(0)\n","# print(name)"]},{"cell_type":"code","execution_count":null,"id":"963f2a03","metadata":{},"outputs":[],"source":["# directory = \"imgs/\"\n","# files_in_directory = os.listdir(directory)\n","# filtered_files = [file for file in files_in_directory if file.endswith(\".webm\")]\n","\n","# for file in filtered_files:\n","# \tpath_to_file = os.path.join(directory, file)\n","# \tos.remove(path_to_file)"]},{"cell_type":"code","execution_count":null,"id":"7a78c028","metadata":{},"outputs":[],"source":["# path = \"imgs/\"\n","# dirs = os.listdir( path )\n","# final_size = 440;\n","\n","# def resize_aspect_fit():\n","#     for item in dirs:\n","#          if item == '.DS_Store':\n","#              continue\n","#          if os.path.isfile(path+item):\n","#              im = Image.open(path+item)\n","#              f, e = os.path.splitext(path+item)\n","#              size = im.size\n","#              ratio = float(final_size) / max(size)\n","#              new_image_size = tuple([int(x*ratio) for x in size])\n","#              im = im.resize(new_image_size, Image.ANTIALIAS)\n","#              new_im = Image.new(\"RGB\", (final_size, final_size))\n","#              new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n","#              new_im.save(f + '_resized.jpg', 'JPEG', quality=90)\n","# resize_aspect_fit()"]},{"cell_type":"code","execution_count":null,"id":"935c04be","metadata":{},"outputs":[],"source":["# directory = \"imgs/\"\n","# files_in_directory = os.listdir(directory)\n","# filtered_files = [file for file in files_in_directory if not file.endswith(\"_resized.jpg\")]\n","\n","# for file in filtered_files:\n","# \tpath_to_file = os.path.join(directory, file)\n","# \tos.remove(path_to_file)"]},{"cell_type":"code","execution_count":null,"id":"61e2b757","metadata":{},"outputs":[],"source":["### THIS WILL CLEAN ALL DATA IN REPLY_TEXT ###\n","\n","exploded_df = dataframe['reply_list'].explode().reset_index()\n","\n","all_reply_test = exploded_df['reply_list']\n","\n","\n","all_replies = []\n","for i in range(0, len(all_reply_test)):\n","    result = re.sub(\"<(.*)>.*?|<(.*) />\", \" \", str(all_reply_test[i]))\n","    result = re.sub('&#039;', \"'\", result)\n","    result = re.sub('&quot;', '\"', result)\n","    all_replies.append(result)"]},{"cell_type":"code","execution_count":null,"id":"570215d9","metadata":{},"outputs":[],"source":["## Append all subject and comment text to lists ##\n","\n","\n","subjects = []\n","comments = []\n","for i in range(0,len(dataframe)):\n","    subject_result =  re.sub(\"<(.*)>.*?|<(.*) />\", \" \", str(dataframe['Subject'][i]))\n","    subject_result = re.sub('&#039;', \"'\", subject_result)\n","    subject_result = re.sub('&quot;', '\"', subject_result)\n","    subjects.append(subject_result)\n","    comment_result =  re.sub(\"<(.*)>.*?|<(.*) />\", \" \", str(dataframe['Comment'][i]))\n","    comment_result = re.sub('&#039;', \"'\", comment_result)\n","    comment_result = re.sub('&quot;', '\"', comment_result)\n","    comments.append(comment_result)\n"]},{"cell_type":"code","execution_count":null,"id":"d8eaf437","metadata":{},"outputs":[],"source":["## ALL TEXT ON THE BOARD ##\n","\n","import numpy as np\n","\n","all_text_on_board = all_replies + subjects + comments\n","\n","textdf = pd.DataFrame({'All text': all_text_on_board})\n","\n","textdf = textdf.replace('0', np.nan)\n","textdf = textdf.replace(' ', np.nan)\n","textdf = textdf.dropna(how='all', axis=0).reset_index(drop=True)\n","\n","textdf"]},{"cell_type":"code","execution_count":null,"id":"8c89978d","metadata":{},"outputs":[],"source":["alltext_list = textdf['All text']\n","\n","literally_all_text =[]\n","for i in range(0,len(alltext_list)):\n","    text = alltext_list[i]\n","    literally_all_text.append(text)\n","\n","corp_string = ' '.join(map(str, literally_all_text))\n","\n","corp_string = re.sub(\"\\d+\", \"\", corp_string)\n","#corp_string = re.sub(r'[\\t\\n ]+', ' ', corp_string)\n","#corp_string = re.sub(r\"\\b([^ ]|\\d+)\\b\",\"\",corp_string)\n","corp_string = re.sub(\"\\\\s+\",\" \", corp_string)\n","corp_string = re.sub(\" \", \" \", corp_string)\n","\n","corp_string.strip()\n","corp_string.lower()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d99c94dd","metadata":{},"outputs":[],"source":["import spacy\n","from collections import Counter\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(corp_string)\n","# all tokens that arent stop words or punctuations\n","words = [token.text\n","         for token in doc\n","         if not token.is_stop and not token.is_punct and len(token) >= 4]\n","\n","# noun tokens that arent stop words or punctuations\n","nouns = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"NOUN\")]\n","\n","adjectives = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"ADJ\")]\n","\n","verbs = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"VERB\")]\n","\n","propn = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"PROPN\")]\n","             \n","adposition = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"ADP\")]\n","\n","adverb = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"ADV\")]\n","pronoun = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"PRON\")]\n","\n","determiner = [token.text\n","         for token in doc\n","         if (not token.is_punct and\n","             token.pos_ == \"DET\")]\n","\n","conjunction = [token.text\n","         for token in doc\n","         if (token.pos_ == \"CONJ\")]\n","\n","\n","# ```ADJ: adjective, e.g. big, old, green, incomprehensible, first\n","# ADP: adposition, e.g. in, to, during\n","# ADV: adverb, e.g. very, tomorrow, down, where, there\n","# AUX: auxiliary, e.g. is, has (done), will (do), should (do)\n","# CONJ: conjunction, e.g. and, or, but\n","# CCONJ: coordinating conjunction, e.g. and, or, but\n","# DET: determiner, e.g. a, an, the\n","# INTJ: interjection, e.g. psst, ouch, bravo, hello\n","# NOUN: noun, e.g. girl, cat, tree, air, beauty\n","# NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV\n","# PART: particle, e.g. ‚Äôs, not,\n","# PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody\n","# PROPN: proper noun, e.g. Mary, John, London, NATO, HBO\n","# PUNCT: punctuation, e.g. ., (, ), ?\n","# SCONJ: subordinating conjunction, e.g. if, while, that\n","# SYM: symbol, e.g. $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù\n","# VERB: verb, e.g. run, runs, running, eat, ate, eating\n","# X: other, e.g. sfpksdpsxmsa\n","# SPACE: space, e.g.````\n","\n","# most common tokens\n","word_freq = Counter(words)\n","common_words = word_freq.most_common(100)\n","\n","# most common noun tokens\n","noun_freq = Counter(nouns)\n","common_nouns = noun_freq.most_common(100)\n","\n","# most common adjective tokens\n","adj_freq = Counter(adjectives)\n","common_adjectives = adj_freq.most_common(100)\n","\n","# most common verb tokens\n","verb_freq = Counter(verbs)\n","common_verbs = verb_freq.most_common(100)\n","\n","# most common propositions\n","prop_freq = Counter(propn)\n","common_props = prop_freq.most_common(100)\n","\n","# most common adpostions\n","adposition_freq = Counter(propn)\n","common_adpositions = adposition_freq.most_common(100)\n","\n","# most common adverbs\n","adverb_freq = Counter(adverb)\n","common_adverbs = adverb_freq.most_common(100)\n","\n","# most common determiners\n","determiner_freq = Counter(determiner)\n","common_determiner = determiner_freq.most_common(100)\n","\n","# most common pronouns\n","pronoun_freq = Counter(pronoun)\n","common_pronouns = pronoun_freq.most_common(100)\n","\n","# most common conjunctions\n","conjunction_freq = Counter(conjunction)\n","common_conjunctions = conjunction_freq.most_common(100)"]},{"cell_type":"code","execution_count":null,"id":"93965aec","metadata":{},"outputs":[],"source":["#len(common_conjunctions)\n","common_conjunctions"]},{"cell_type":"code","execution_count":null,"id":"7d1f1131","metadata":{},"outputs":[],"source":["# Sample sentences\n","x = 10\n","y = len(common_pronouns)\n","z = len(common_conjunctions)\n","\n","\n","for i in range(0,7):\n","    sentence = common_props[(random.randrange(0, x, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0] + \" \" +  common_adjectives[(random.randrange(0, x, 1))][0]+ \" \" + common_nouns[(random.randrange(0, x, 1))][0]\n","    sentence2 = common_props[(random.randrange(0, x, 1))][0]+ \" \" + common_nouns[(random.randrange(0, x, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0] + \" \" +  common_adjectives[(random.randrange(0, x, 1))][0] + \" \" + common_nouns[(random.randrange(0, x, 1))][0]\n","    sentence3 = common_pronouns[(random.randrange(0, y, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0]+ \" \" + common_adjectives[(random.randrange(0, x, 1))][0] + \" \" + common_nouns[(random.randrange(0, x, 1))][0]+ \" \" + common_pronouns[(random.randrange(0, y, 1))][0]+ \" \" + common_verbs[(random.randrange(0, x, 1))][0]+ \" \" + common_pronouns[(random.randrange(0, y, 1))][0]\n","\n","    print(sentence)\n","    print(sentence2)\n"]},{"cell_type":"code","execution_count":null,"id":"67f3c408","metadata":{},"outputs":[],"source":["from bs4 import BeautifulSoup\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Wordcloud creation\n","\n","# Get all common words, map to string for wordcloud\n","wordcloud_common = []\n","for i in range(0,100):\n","    word = common_words[i][0]\n","    wordcloud_common.append(word)\n","\n","wordcloud_string = ' '.join(map(str, wordcloud_common))\n","\n","#corp_string_wordcloud = re.sub(\"https\", \"\", corp_string)\n","\n","wordcloud = WordCloud(width = 800, height = 800,\n","                background_color = 'white',\n","                stopwords=STOPWORDS,\n","                min_font_size = 10).generate(wordcloud_string)\n"," \n","# plot the WordCloud image                      \n","plt.figure(figsize = (8, 8), facecolor = None)\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad = 10)\n"," \n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"8e88c1e9","metadata":{},"outputs":[],"source":["corp_string"]},{"cell_type":"code","execution_count":null,"id":"d77e857c","metadata":{},"outputs":[],"source":["# 'BAD WORDS' list to try data on GPT-3\n","# https://github.com/web-mech/badwords\n","url = ('https://raw.githubusercontent.com/web-mech/badwords/master/lib/lang.json')\n","data = get_jsonparsed_data(url)\n","\n","badwords = []\n","for i in range(0, len(data['words'])):\n","    word  = data['words'][i]\n","    badwords.append(word)\n","\n","\n","text = corp_string\n","for badword in badwords:\n","        text = text.replace(badword,\"\"*len(badword))\n","\n","text\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f5bc7f4d","metadata":{},"outputs":[],"source":["split_text = text.split()\n","random.shuffle(split_text)\n","\n","split_text\n","\n","newtext = ' '.join(map(str, split_text))\n","\n","newtext[:200]"]},{"cell_type":"code","execution_count":null,"id":"e7fddca7","metadata":{},"outputs":[],"source":["## Explaining this board to a second grader\n","\n","import os\n","import openai\n","\n","openai.api_key = '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'\n","\n","response = openai.Completion.create(\n","  engine=\"davinci\",\n","  prompt=\"My second grader asked me what this passage means:\\n\\\"\\\"\\\"\\n\" + newtext[:800] + \"\\\"\\\"\\\"\\nI rephrased it for him, in plain language a fourth grader can understand:\\n\\\"\\\"\\\"\\n\",\n","  temperature=0.6,\n","  max_tokens=280,\n","  top_p=1,\n","  frequency_penalty=0.2,\n","  presence_penalty=0,\n","  stop=[\"\\\"\\\"\\\"\"]\n",")\n","\n","print(response['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"id":"32728fb5","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"biznotebook.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}
