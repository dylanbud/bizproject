{"cells":[{"cell_type":"markdown","id":"c05d1067","metadata":{},"source":["## 4chan Scraper ##\n","\n","This notebook will scrape a given board on 4chan at the moment, create a wordcloud of the most common words, and if desired, download all images associated with the 200 active threads and x number of replies on the board.\n","\n","The idea behind this scraping is to feed the reply/subject text data into a machine learning model to analyze posts. The project came out of the desire to analyze the /biz/ board to determine which cryptocurrencies were becoming popular/which cryptocurrencies had a sustained buzz around them. This information could then be utilized in combination with a token sniper or other web3 application for a slight advantage in trading.\n","\n","Image wise, this aspect is not fully fleshed out. The idea was to use neural networks to analyze and categorize types of memes on boards, but I have not had the time to train a model and am waiting for access to GPT-3's DALL-E rather than using what transformers are currently available. \n","\n","The image data is collected and stored in /imgs/, I'd like for the images to eventually be the input for both a trained classification neural network and a trained generative neural network."]},{"cell_type":"code","execution_count":1,"id":"5b172e0c","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Title</th>\n","      <th>Combined</th>\n","      <th>Pages</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>3DCG</td>\n","      <td>3 3DCG</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>a</td>\n","      <td>Anime &amp; Manga</td>\n","      <td>a Anime &amp; Manga</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>aco</td>\n","      <td>Adult Cartoons</td>\n","      <td>aco Adult Cartoons</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>adv</td>\n","      <td>Advice</td>\n","      <td>adv Advice</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>an</td>\n","      <td>Animals &amp; Nature</td>\n","      <td>an Animals &amp; Nature</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>wsg</td>\n","      <td>Worksafe GIF</td>\n","      <td>wsg Worksafe GIF</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>wsr</td>\n","      <td>Worksafe Requests</td>\n","      <td>wsr Worksafe Requests</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>x</td>\n","      <td>Paranormal</td>\n","      <td>x Paranormal</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>xs</td>\n","      <td>Extreme Sports</td>\n","      <td>xs Extreme Sports</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>y</td>\n","      <td>Yaoi</td>\n","      <td>y Yaoi</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>78 rows × 4 columns</p>\n","</div>"],"text/plain":["   Name              Title               Combined  Pages\n","0     3               3DCG                 3 3DCG     10\n","1     a      Anime & Manga        a Anime & Manga     10\n","2   aco     Adult Cartoons     aco Adult Cartoons     10\n","3   adv             Advice             adv Advice     10\n","4    an   Animals & Nature    an Animals & Nature     10\n","..  ...                ...                    ...    ...\n","73  wsg       Worksafe GIF       wsg Worksafe GIF     10\n","74  wsr  Worksafe Requests  wsr Worksafe Requests     10\n","75    x         Paranormal           x Paranormal     10\n","76   xs     Extreme Sports      xs Extreme Sports     10\n","77    y               Yaoi                 y Yaoi     10\n","\n","[78 rows x 4 columns]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import time\n","import pandas as pd\n","from urllib.request import urlopen\n","import json\n","from bs4 import BeautifulSoup\n","from urllib.error import HTTPError\n","import requests\n","from PIL import Image\n","import re\n","import random\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import matplotlib.pyplot as plt\n","import os\n","import openai\n","import spacy\n","from collections import Counter\n","\n","openai.api_key = \"OPEN-API-KEY\"\n","\n","def get_jsonparsed_data(url):\n","    response = urlopen(url)\n","    data = response.read().decode(\"utf-8\")\n","    return json.loads(data)\n","\n","# Collect general information about boards\n","\n","url = (('https://a.4cdn.org/boards.json'))\n","\n","data = get_jsonparsed_data(url)\n","\n","\n","board_names = []\n","board_titles = []\n","combined = []\n","pages = []\n","\n","for i in range(0, 78):\n","    name = data['boards'][i]['board']\n","    title = data['boards'][i]['title']\n","    page = data['boards'][i]['pages']\n","    board_names.append(name)\n","    board_titles.append(title)\n","    pages.append(page)\n","    combination = name + \" \" + title\n","    combined.append(combination)\n","\n","board_df = pd.DataFrame({'Name' : board_names, 'Title' : board_titles, 'Combined' : combined, 'Pages' : pages})\n","\n","board_df"]},{"cell_type":"code","execution_count":2,"id":"99bd18ba","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["biz\n"]}],"source":["# prompt user for input\n","board = input(\"Enter a board: \")\n","print(board)"]},{"cell_type":"code","execution_count":3,"id":"824a476c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":2031,"status":"ok","timestamp":1639433463624,"user":{"displayName":"Dylan Budnick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhX3fJsQsvqzuZK06O7pK0XWM30sB0MVv0a0BZ3Hg=s64","userId":"08607160935893473835"},"user_tz":480},"id":"824a476c","outputId":"790ffc4b-1a51-4d27-d5ae-0c00d4807881"},"outputs":[],"source":["# Pulling Board information\n","\n","\n","def get_jsonparsed_data(url):\n","    response = urlopen(url)\n","    data = response.read().decode(\"utf-8\")\n","    return json.loads(data)\n","\n","\n","def get_numbers(df, i):  \n","    postno = df['Post Number'][i]\n","    replies = df['Replies'][i]\n","    return(postno, replies)\n","\n","dataframe = pd.DataFrame(columns=['Subject', 'Comment', 'Post Number', 'Replies', 'reply_list', 'filename', 'tim_list', 'ext_list', 'combined'])\n","for i in range(10):\n","    i=i+1\n","    url = ((\"https://a.4cdn.org/\") + str(board) + \"/\" + str(i) + '.json')\n","    data = get_jsonparsed_data(url)\n","\n","    list = len(data['threads'])\n","   \n","    for i in range(0, list): \n","      try:\n","          comment = data['threads'][i]['posts'][0]['com']\n","          subject = data['threads'][i]['posts'][0]['sub']\n","          postno = data['threads'][i]['posts'][0]['no']\n","          replies = data['threads'][i]['posts'][0]['replies']\n","      except KeyError:\n","          subject = 'No subject'\n","          postno = data['threads'][i]['posts'][0]['no']\n","          replies = data['threads'][i]['posts'][0]['replies']\n","      except KeyError:\n","          comment = data['threads'][i]['posts'][0]['sub']\n","      dataframe = dataframe.append({'Subject':subject, 'Comment':comment, 'Post Number':postno, 'Replies':replies}, ignore_index=True)\n","    time.sleep(1)\n","    i=i+1\n","\n","dataframe = dataframe.fillna(0)"]},{"cell_type":"markdown","id":"1c117af2","metadata":{},"source":["Above code produces dataframe seen below"]},{"cell_type":"code","execution_count":4,"id":"dd1c8f24","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>Comment</th>\n","      <th>Post Number</th>\n","      <th>Replies</th>\n","      <th>reply_list</th>\n","      <th>filename</th>\n","      <th>tim_list</th>\n","      <th>ext_list</th>\n","      <th>combined</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NO BEGGING</td>\n","      <td>&lt;span style=\"font-weight:600;font-size:150%;li...</td>\n","      <td>4884770</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Welcome to /biz/ - Business &amp;amp; Finance</td>\n","      <td>This board is for the discussion of topics rel...</td>\n","      <td>21374000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>No subject</td>\n","      <td>F</td>\n","      <td>45083736</td>\n","      <td>1000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/AVAX/ - Avalanche General #278</td>\n","      <td>First of the Year Edition&lt;br&gt;&lt;br&gt;&lt;span class=\"...</td>\n","      <td>45090694</td>\n","      <td>288</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/smg/ stock market general</td>\n","      <td>goodnight sweet prince&lt;br&gt;&lt;br&gt;&lt;span class=\"quo...</td>\n","      <td>45105547</td>\n","      <td>86</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     Subject  \\\n","0                                 NO BEGGING   \n","1  Welcome to /biz/ - Business &amp; Finance   \n","2                                 No subject   \n","3            /AVAX/ - Avalanche General #278   \n","4                 /smg/ stock market general   \n","\n","                                             Comment  Post Number  Replies  \\\n","0  <span style=\"font-weight:600;font-size:150%;li...      4884770        0   \n","1  This board is for the discussion of topics rel...     21374000        1   \n","2                                                  F     45083736     1000   \n","3  First of the Year Edition<br><br><span class=\"...     45090694      288   \n","4  goodnight sweet prince<br><br><span class=\"quo...     45105547       86   \n","\n","   reply_list  filename  tim_list  ext_list  combined  \n","0           0         0         0         0         0  \n","1           0         0         0         0         0  \n","2           0         0         0         0         0  \n","3           0         0         0         0         0  \n","4           0         0         0         0         0  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataframe.head(5)"]},{"cell_type":"code","execution_count":5,"id":"891dd3f1","metadata":{},"outputs":[],"source":["import sqlite3\n","con = sqlite3.connect('pdx911.sqlite3')\n","c = con.cursor()\n"]},{"cell_type":"code","execution_count":6,"id":"fec1a456","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Bernie\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2882: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n","  method=method,\n"]}],"source":["dataframe.to_sql('board1', con=con, if_exists='append', index=False)"]},{"cell_type":"markdown","id":"7df1214d","metadata":{},"source":["### Grab reply text, image md5 hash (name), and img extension #\n","\n","This function goes in and grabs all text and image data from replies in a given thread. It then drops the replies into a list in \"reply_list\", and drops the image name (it's md5 hash) and extension in the repsective columns\n","called \"tim_list\" and \"ext_list\". The \"combined\" column is the result of pairing the img name (md5) with the correlating extension.\n"]},{"cell_type":"code","execution_count":7,"id":"KOubLYwT12YX","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1639434665151,"user":{"displayName":"Dylan Budnick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhX3fJsQsvqzuZK06O7pK0XWM30sB0MVv0a0BZ3Hg=s64","userId":"08607160935893473835"},"user_tz":480},"id":"KOubLYwT12YX","outputId":"02a4dadf-68f6-4936-8767-3d0afe658380"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Bernie\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","C:\\Users\\Bernie\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_block(indexer, value, name)\n","C:\\Users\\Bernie\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","C:\\Users\\Bernie\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","C:\\Users\\Bernie\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}],"source":["for i in range(0,len(dataframe)):\n","\n","  try:\n","    postno, replies = get_numbers(dataframe, i) \n","    url = ((\"https://a.4cdn.org/\" + str(board) + \"/thread/\") + str(postno) + '.json')\n","    data = get_jsonparsed_data(url)\n","    replies_text = []\n","    extensions = []\n","    images = []\n","    combined = []\n","    filenames = []\n","    for j in range(0,replies):\n","      try:\n","        reply = data['posts'][j]['com']\n","        img = data['posts'][j]['tim']\n","        ext = data['posts'][j]['ext']\n","        #file = data['posts'][i]['filename']\n","        #filenames.append(file)\n","        replies_text.append(reply)\n","        images.append(str(img))\n","        extensions.append(str(ext))\n","        combined.append(str(img)+str(ext))\n","        dataframe['reply_list'][i] = replies_text\n","        dataframe['tim_list'][i] = images\n","        dataframe['ext_list'][i] = extensions\n","        dataframe['combined'][i] = combined\n","        #dataframe['filename'][i] = filenames\n","      except KeyError:\n","        pass\n","      except IndexError: \n","        pass\n","  except HTTPError:\n","    dataframe['reply_list'][i] = '404'\n","    dataframe['tim_list'][i] = '404'\n","    dataframe['ext_list'][i] = '404'\n","  else:\n","    pass\n","time.sleep(1)\n"]},{"cell_type":"markdown","id":"2caad79f","metadata":{},"source":["The above will format the dataframe into the format below:"]},{"cell_type":"code","execution_count":8,"id":"87d3f7e2","metadata":{},"outputs":[],"source":["dataframe2 = dataframe.to_json(orient = 'split')"]},{"cell_type":"code","execution_count":9,"id":"992b2052","metadata":{},"outputs":[{"data":{"text/plain":["'m'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataframe2[6][0]"]},{"cell_type":"code","execution_count":10,"id":"9c35fcaa","metadata":{},"outputs":[{"ename":"InterfaceError","evalue":"Error binding parameter 4 - probably unsupported type.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-10-4335d1305f70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'4chan.sqlite3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'boards'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2880\u001b[0m             \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2881\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2882\u001b[1;33m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2883\u001b[0m         )\n\u001b[0;32m   2884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, **kwargs)\u001b[0m\n\u001b[0;32m   2224\u001b[0m         )\n\u001b[0;32m   2225\u001b[0m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2226\u001b[1;33m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhas_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, chunksize, method)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m                 \u001b[0mexec_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     def _query_iterator(\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[1;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[0;32m   1922\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_statement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mInterfaceError\u001b[0m: Error binding parameter 4 - probably unsupported type."]}],"source":["import sqlite3\n","con = sqlite3.connect('4chan.sqlite3')\n","c = con.cursor()\n","dataframe.to_sql('boards', con=con, if_exists='append', index=False)"]},{"cell_type":"code","execution_count":null,"id":"ec9bf4c1","metadata":{},"outputs":[],"source":["# # DOWNLOAD ALL IMAGES FOR THREAD/IN THREADS\n","\n","# links = []\n","# newlinks = []\n","# dataframe['combined'] = dataframe['combined'].fillna(0)\n","# combined = dataframe['combined']\n","\n","# for i in range(0, len(combined)):\n","#     if combined[i] == 0:\n","#         links.append('No image')\n","#     elif combined[i] != 0:\n","#         links = combined[i]\n","#         for i in links:\n","#             newlinks.append(i)\n","#             imgURL = (\"https://i.4cdn.org/biz/\") + str(i)\n","#             name = str(i)\n","#             print(imgURL)\n","#             r = requests.get(imgURL)\n","#             with open('imgs/'+name, 'wb') as f:\n","#                 f.write(r.content)\n","#             time.sleep(0)\n","# print(name)"]},{"cell_type":"code","execution_count":null,"id":"963f2a03","metadata":{},"outputs":[],"source":["# directory = \"imgs/\"\n","# files_in_directory = os.listdir(directory)\n","# filtered_files = [file for file in files_in_directory if file.endswith(\".webm\")]\n","\n","# for file in filtered_files:\n","# \tpath_to_file = os.path.join(directory, file)\n","# \tos.remove(path_to_file)"]},{"cell_type":"code","execution_count":null,"id":"7a78c028","metadata":{},"outputs":[],"source":["# path = \"imgs/\"\n","# dirs = os.listdir( path )\n","# final_size = 440;\n","\n","# def resize_aspect_fit():\n","#     for item in dirs:\n","#          if item == '.DS_Store':\n","#              continue\n","#          if os.path.isfile(path+item):\n","#              im = Image.open(path+item)\n","#              f, e = os.path.splitext(path+item)\n","#              size = im.size\n","#              ratio = float(final_size) / max(size)\n","#              new_image_size = tuple([int(x*ratio) for x in size])\n","#              im = im.resize(new_image_size, Image.ANTIALIAS)\n","#              new_im = Image.new(\"RGB\", (final_size, final_size))\n","#              new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n","#              new_im.save(f + '_resized.jpg', 'JPEG', quality=90)\n","# resize_aspect_fit()"]},{"cell_type":"code","execution_count":null,"id":"935c04be","metadata":{},"outputs":[],"source":["# directory = \"imgs/\"\n","# files_in_directory = os.listdir(directory)\n","# filtered_files = [file for file in files_in_directory if not file.endswith(\"_resized.jpg\")]\n","\n","# for file in filtered_files:\n","# \tpath_to_file = os.path.join(directory, file)\n","# \tos.remove(path_to_file)"]},{"cell_type":"code","execution_count":null,"id":"61e2b757","metadata":{},"outputs":[],"source":["### THIS WILL CLEAN ALL DATA IN REPLY_TEXT ###\n","\n","exploded_df = dataframe['reply_list'].explode().reset_index()\n","\n","all_reply_test = exploded_df['reply_list']\n","\n","\n","all_replies = []\n","for i in range(0, len(all_reply_test)):\n","    result = re.sub(\"<(.*)>.*?|<(.*) />\", \" \", str(all_reply_test[i]))\n","    result = re.sub('&#039;', \"'\", result)\n","    result = re.sub('&quot;', '\"', result)\n","    all_replies.append(result)"]},{"cell_type":"code","execution_count":null,"id":"570215d9","metadata":{},"outputs":[],"source":["## Append all subject and comment text to lists ##\n","\n","\n","subjects = []\n","comments = []\n","for i in range(0,len(dataframe)):\n","    subject_result =  re.sub(\"<(.*)>.*?|<(.*) />\", \" \", str(dataframe['Subject'][i]))\n","    subject_result = re.sub('&#039;', \"'\", subject_result)\n","    subject_result = re.sub('&quot;', '\"', subject_result)\n","    subjects.append(subject_result)\n","    comment_result =  re.sub(\"<(.*)>.*?|<(.*) />\", \" \", str(dataframe['Comment'][i]))\n","    comment_result = re.sub('&#039;', \"'\", comment_result)\n","    comment_result = re.sub('&quot;', '\"', comment_result)\n","    comments.append(comment_result)\n"]},{"cell_type":"code","execution_count":null,"id":"d8eaf437","metadata":{},"outputs":[],"source":["## ALL TEXT ON THE BOARD ##\n","\n","import numpy as np\n","\n","all_text_on_board = all_replies + subjects + comments\n","\n","textdf = pd.DataFrame({'All text': all_text_on_board})\n","\n","textdf = textdf.replace('0', np.nan)\n","textdf = textdf.replace(' ', np.nan)\n","textdf = textdf.dropna(how='all', axis=0).reset_index(drop=True)\n","\n","textdf"]},{"cell_type":"code","execution_count":null,"id":"8c89978d","metadata":{},"outputs":[],"source":["alltext_list = textdf['All text']\n","\n","literally_all_text =[]\n","for i in range(0,len(alltext_list)):\n","    text = alltext_list[i]\n","    literally_all_text.append(text)\n","\n","corp_string = ' '.join(map(str, literally_all_text))\n","\n","corp_string = re.sub(\"\\d+\", \"\", corp_string)\n","#corp_string = re.sub(r'[\\t\\n ]+', ' ', corp_string)\n","#corp_string = re.sub(r\"\\b([^ ]|\\d+)\\b\",\"\",corp_string)\n","corp_string = re.sub(\"\\\\s+\",\" \", corp_string)\n","corp_string = re.sub(\" \", \" \", corp_string)\n","\n","corp_string.strip()\n","corp_string.lower()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d99c94dd","metadata":{},"outputs":[],"source":["corp_string = corp_string.lower()\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(corp_string)\n","STOPWORDS = [\"coin\", \"shit\", \"time\", \"year\", \"day\", \"fuck\", \"life\", \"way\", \"guys\", \"%\", \"days\", \"bros\", \"pic\", \"bull\", \"dump\", \"bags\", \"man\", \"rugs\", \"run\", \"lot\", \"fees\", \"home\", \"guy\", \"idea\", \"k\", \"moon\", \"pump\", \"bag\", \"scam\"]\n","\n","# all tokens that arent stop words or punctuations\n","words = [token.text\n","         for token in doc\n","         if not token.is_stop and not token.is_punct and len(token) >= 3]\n","\n","# noun tokens that arent stop words or punctuations\n","nouns = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"NOUN\")]\n","\n","adjectives = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"ADJ\")]\n","\n","verbs = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"VERB\")]\n","\n","propn = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"PROPN\")]\n","             \n","adposition = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"ADP\")]\n","\n","adverb = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"ADV\")]\n","pronoun = [token.text\n","         for token in doc\n","         if (not token.is_stop and\n","             not token.is_punct and\n","             token.pos_ == \"PRON\")]\n","\n","determiner = [token.text\n","         for token in doc\n","         if (not token.is_punct and\n","             token.pos_ == \"DET\")]\n","\n","conjunction = [token.text\n","         for token in doc\n","         if (token.pos_ == \"CONJ\")]\n","\n","potential_coins = [token.text\n","         for token in doc\n","         if (not token.is_stop \n","         and token.pos_ == \"NOUN\"\n","         and len(token) <= 4)]\n","\n","\n","# ```ADJ: adjective, e.g. big, old, green, incomprehensible, first\n","# ADP: adposition, e.g. in, to, during\n","# ADV: adverb, e.g. very, tomorrow, down, where, there\n","# AUX: auxiliary, e.g. is, has (done), will (do), should (do)\n","# CONJ: conjunction, e.g. and, or, but\n","# CCONJ: coordinating conjunction, e.g. and, or, but\n","# DET: determiner, e.g. a, an, the\n","# INTJ: interjection, e.g. psst, ouch, bravo, hello\n","# NOUN: noun, e.g. girl, cat, tree, air, beauty\n","# NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV\n","# PART: particle, e.g. ’s, not,\n","# PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody\n","# PROPN: proper noun, e.g. Mary, John, London, NATO, HBO\n","# PUNCT: punctuation, e.g. ., (, ), ?\n","# SCONJ: subordinating conjunction, e.g. if, while, that\n","# SYM: symbol, e.g. $, %, §, ©, +, −, ×, ÷, =, :), 😝\n","# VERB: verb, e.g. run, runs, running, eat, ate, eating\n","# X: other, e.g. sfpksdpsxmsa\n","# SPACE: space, e.g.````\n","\n","# most common tokens\n","word_freq = Counter(words)\n","common_words = word_freq.most_common(100)\n","\n","# most common noun tokens\n","noun_freq = Counter(nouns)\n","common_nouns = noun_freq.most_common(100)\n","\n","# most common adjective tokens\n","adj_freq = Counter(adjectives)\n","common_adjectives = adj_freq.most_common(100)\n","\n","# most common verb tokens\n","verb_freq = Counter(verbs)\n","common_verbs = verb_freq.most_common(100)\n","\n","# most common propositions\n","prop_freq = Counter(propn)\n","common_props = prop_freq.most_common(100)\n","\n","# most common adpostions\n","adposition_freq = Counter(propn)\n","common_adpositions = adposition_freq.most_common(100)\n","\n","# most common adverbs\n","adverb_freq = Counter(adverb)\n","common_adverbs = adverb_freq.most_common(100)\n","\n","# most common determiners\n","determiner_freq = Counter(determiner)\n","common_determiner = determiner_freq.most_common(100)\n","\n","# most common pronouns\n","pronoun_freq = Counter(pronoun)\n","common_pronouns = pronoun_freq.most_common(100)\n","\n","# most common conjunctions\n","conjunction_freq = Counter(conjunction)\n","common_conjunctions = conjunction_freq.most_common(100)\n","\n","potential_coins_freq = Counter(potential_coins)\n","common_potential_coins = potential_coins_freq.most_common(100)\n"]},{"cell_type":"code","execution_count":null,"id":"93965aec","metadata":{},"outputs":[],"source":["#potential_coins\n","from nltk.probability import FreqDist\n","fdist = FreqDist(potential_coins)\n","common_potential_coins\n"]},{"cell_type":"code","execution_count":null,"id":"dbd666f2","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7d1f1131","metadata":{},"outputs":[],"source":["# Sample sentences\n","x = 10\n","y = len(common_pronouns)\n","z = len(common_conjunctions)\n","\n","\n","for i in range(0,7):\n","    sentence = common_props[(random.randrange(0, x, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0] + \" \" +  common_adjectives[(random.randrange(0, x, 1))][0]+ \" \" + common_nouns[(random.randrange(0, x, 1))][0]\n","    sentence2 = common_props[(random.randrange(0, x, 1))][0]+ \" \" + common_nouns[(random.randrange(0, x, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0] + \" \" +  common_adjectives[(random.randrange(0, x, 1))][0] + \" \" + common_nouns[(random.randrange(0, x, 1))][0]\n","    sentence3 = common_pronouns[(random.randrange(0, y, 1))][0] + \" \" + common_verbs[(random.randrange(0, x, 1))][0]+ \" \" + common_adjectives[(random.randrange(0, x, 1))][0] + \" \" + common_nouns[(random.randrange(0, x, 1))][0]+ \" \" + common_pronouns[(random.randrange(0, y, 1))][0]+ \" \" + common_verbs[(random.randrange(0, x, 1))][0]+ \" \" + common_pronouns[(random.randrange(0, y, 1))][0]\n","\n","    print(sentence)\n","    print(sentence2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Wordcloud creation\n","from datetime import datetime\n","now = datetime.now()\n","dt_string = now.strftime(\"%m%d-%Y%H%M%S\")\n","\n","# Get all common words, map to string for wordcloud\n","wordcloud_common = []\n","for i in range(0,100):\n","    word = common_words[i][0]\n","    wordcloud_common.append(word)\n","\n","wordcloud_string = ' '.join(map(str, wordcloud_common))\n","\n","#corp_string_wordcloud = re.sub(\"https\", \"\", corp_string)\n","\n","wordcloud = WordCloud(width = 800, height = 800,\n","                background_color = 'white',\n","                stopwords=STOPWORDS,\n","                min_font_size = 10).generate(wordcloud_string)\n"," \n","# plot the WordCloud image    \n","                  \n","plt.figure(figsize = (10, 10), facecolor = None)\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad = 10)\n","plt.savefig(board + \"\" + dt_string + \"\" + \".png\", format=\"png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"b9d5b2e1","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"8e88c1e9","metadata":{},"outputs":[],"source":["corp_string"]},{"cell_type":"code","execution_count":null,"id":"d77e857c","metadata":{},"outputs":[],"source":["# 'BAD WORDS' list to try data on GPT-3\n","# https://github.com/web-mech/badwords\n","\n","url = ('https://raw.githubusercontent.com/web-mech/badwords/master/lib/lang.json')\n","data = get_jsonparsed_data(url)\n","\n","badwords = []\n","for i in range(0, len(data['words'])):\n","    word  = data['words'][i]\n","    badwords.append(word)\n","\n","\n","text = corp_string\n","for badword in badwords:\n","        text = text.replace(badword,\"\"*len(badword))\n","\n","text\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f5bc7f4d","metadata":{},"outputs":[],"source":["split_text = text.split()\n","random.shuffle(split_text)\n","\n","split_text\n","\n","newtext = ' '.join(map(str, split_text))\n","\n","newtext[:200]"]},{"cell_type":"code","execution_count":null,"id":"8adaa184","metadata":{},"outputs":[],"source":["newtext[:800]"]},{"cell_type":"code","execution_count":null,"id":"e7fddca7","metadata":{},"outputs":[],"source":["## Explaining this board to a second grader\n","## Uses OPENAI\n","\n","response = openai.Completion.create(\n","  engine=\"davinci\",\n","  prompt=\"My fourth grader asked me what this passage means:\\n\\\"\\\"\\\"\\n\" + newtext[:800] + \"\\\"\\\"\\\"\\nI rephrased it for him, in plain language a fourth grader can understand:\\n\\\"\\\"\\\"\\n\",\n","  temperature=0.6,\n","  max_tokens=280,\n","  top_p=1,\n","  frequency_penalty=0.2,\n","  presence_penalty=0,\n","  stop=[\"\\\"\\\"\\\"\"]\n",")\n","\n","print(response['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"id":"32728fb5","metadata":{},"outputs":[],"source":["plt.savefig(board + \"\" + '.png')\n","\n","\n","# def create_wordcloud():\n","# ''' Wordcloud creation:\n","\n","# Get all common words from the board, map to string for wordcloud, generate a wordcloud \n","# '''\n","\n","# wordcloud_common = []\n","# for i in range(0,100):\n","#     word = common_words[i][0]\n","#     wordcloud_common.append(word)\n","\n","# wordcloud_string = ' '.join(map(str, wordcloud_common))\n","\n","# #corp_string_wordcloud = re.sub(\"https\", \"\", corp_string)\n","\n","# wordcloud = WordCloud(width = 800, height = 800,\n","#                 background_color = 'white',\n","#                 stopwords=STOPWORDS,\n","#                 min_font_size = 10).generate(wordcloud_string)\n"," \n","# # plot the WordCloud image                      \n","# plt.figure(figsize = (8, 8), facecolor = None)\n","# plt.imshow(wordcloud)\n","# plt.axis(\"off\")\n","# plt.tight_layout(pad = 10)\n"," \n","# plt.show()\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"biznotebook.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}
